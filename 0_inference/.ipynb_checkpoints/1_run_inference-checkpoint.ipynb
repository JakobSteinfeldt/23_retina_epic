{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/jakobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_md = pd.read_csv(f\"{base_path}/BiHealth/onnx/endpoints.csv\").drop(columns=\"Unnamed: 0\").set_index(\"endpoint\")#[[\"endpoint\", \"eligable\", \"n\", \"freq\", \"phecode\", \"phecode_string\", \"phecode_category\", \"sex\"]]\n",
    "endpoints_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @JAKOB: If loading the hydra config is a problem I've also put the actual\n",
    "# params in the comments so you don't necessarily have to use hydra. \n",
    "\n",
    "import hydra\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "hydra.initialize(config_path=\"../../RetinalRisk/config\")\n",
    "\n",
    "covariates = \"no_covariates\"\n",
    "num_hidden = 256\n",
    "\n",
    "cfg = hydra.compose(\n",
    "    config_name=\"config\",\n",
    "    overrides=[\n",
    "        \"training.gradient_checkpointing=False\",\n",
    "        f\"datamodule/covariates={covariates}\",\n",
    "        \"datamodule.partition=20\",\n",
    "        \"model=retfound\",\n",
    "        \"model.retfound_augment=True\",\n",
    "        \"setup.use_data_artifact_if_available=False\",\n",
    "        \"head=mlp\",\n",
    "        f\"head.kwargs.num_hidden={num_hidden}\",\n",
    "        \"head.kwargs.num_layers=2\",\n",
    "        \"head.dropout=0\",\n",
    "        \"training.optimizer_kwargs.weight_decay=0.001\",\n",
    "        \"training.optimizer_kwargs.lr=0.0001\",\n",
    "        \"model.freeze_encoder=False\",\n",
    "        \"datamodule.batch_size=32\",\n",
    "        \"training.warmup_period=8\",\n",
    "        \"datamodule/augmentation=contrast_sharpness_posterize\",\n",
    "        \"datamodule.img_size_to_gpu=224\",\n",
    "        \"datamodule.num_workers=32\",\n",
    "        \"model.pretrained=True\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinalrisk.models.retfound import vit_large_patch16, interpolate_pos_embed\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "def setup_retfound_model(args):\n",
    "    checkpoint_path = args.model.checkpoint_path\n",
    "\n",
    "    image_size = args.datamodule.augmentation.train.CenterCrop.size\n",
    "\n",
    "    model = vit_large_patch16(\n",
    "        num_classes=2,\n",
    "        drop_path_rate=args.model.drop_path_rate,\n",
    "        global_pool=True,\n",
    "        img_size=image_size,\n",
    "    )\n",
    "\n",
    "    if args.model.pretrained:\n",
    "        # load RETFound weights\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        checkpoint_model = checkpoint[\"model\"]\n",
    "        state_dict = model.state_dict()\n",
    "        for k in [\"head.weight\", \"head.bias\"]:\n",
    "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # interpolate position embedding\n",
    "        interpolate_pos_embed(model, checkpoint_model)\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "\n",
    "        assert set(msg.missing_keys) == {\n",
    "            \"head.weight\",\n",
    "            \"head.bias\",\n",
    "            \"fc_norm.weight\",\n",
    "            \"fc_norm.bias\",\n",
    "        }\n",
    "\n",
    "    # manually initialize fc layer\n",
    "    trunc_normal_(model.head.weight, std=2e-5)\n",
    "\n",
    "    encoder = model\n",
    "    encoder.head = torch.nn.Identity()\n",
    "\n",
    "    outshape = 1024\n",
    "\n",
    "    return encoder, outshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from socket import gethostname\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchmetrics\n",
    "\n",
    "from retinalrisk.models.supervised import (\n",
    "    ImageTraining\n",
    ")\n",
    "from retinalrisk.modules.head import MLPHead\n",
    "\n",
    "def setup_training():\n",
    "    \n",
    "    def get_head(num_head_features, num_endpoints):\n",
    "\n",
    "        cls = MLPHead\n",
    "\n",
    "        return cls(\n",
    "            num_head_features,\n",
    "            num_endpoints,\n",
    "            incidence=None,\n",
    "            dropout=cfg.head.dropout, # 0\n",
    "            gradient_checkpointing=False,\n",
    "            num_hidden = cfg.head.kwargs.num_hidden, # 256\n",
    "            num_layers = cfg.head.kwargs.num_layers, # 2\n",
    "        )\n",
    "    \n",
    "    # base_path = \"/home/jakobs\"\n",
    "    x = torch.load(f\"{base_path}/BiHealth/ckpts_RetFound/4.ckpt\", map_location=torch.device('cpu'))\n",
    "    #x = torch.load('/sc-projects/sc-proj-ukb-cvd/results/models/retina/2cpw9zcx/checkpoints/epoch=67-step=13464.ckpt', map_location=torch.device('cpu'))\n",
    "    \n",
    "    losses = x['hyper_parameters'][\"losses\"]\n",
    "    label_mapping = x['hyper_parameters'][\"label_mapping\"]\n",
    "    incidence_mapping = x['hyper_parameters'][\"incidence_mapping\"]\n",
    "\n",
    "    cfg.model.checkpoint_path = f\"{base_path}/BiHealth/ckpts_RetFound/RETFound_cfp_weights.pth\"\n",
    "    encoder, outshape = setup_retfound_model(cfg)\n",
    "\n",
    "    head = get_head(num_head_features = outshape, num_endpoints = 773)\n",
    "\n",
    "    model = ImageTraining(encoder=encoder, head=head, losses=losses, label_mapping=label_mapping, incidence_mapping=None, metrics_list=[], task=\"tte\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import PIL\n",
    "from typing import Union\n",
    "from random import choice\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.io import imread\n",
    "\n",
    "from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "\n",
    "img_size_to_gpu = cfg.datamodule.augmentation.train.CenterCrop.size # 256\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        RandomResizedCropAndInterpolation(\n",
    "            size=(img_size_to_gpu, img_size_to_gpu),\n",
    "            scale=(0.08, 1.0),\n",
    "            ratio=(1, 1),\n",
    "            interpolation=\"bicubic\",\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    data: torch.Tensor\n",
    "    covariates: torch.Tensor\n",
    "    names: list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Inference itself okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ckpt = torch.load(f'{base_path}/BiHealth/ckpts_RetFound/4.ckpt', map_location='cpu')\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval();\n",
    "model.to(\"cuda\")\n",
    "\n",
    "batch_hpc = pickle.load(open(f'{base_path}/data/epic_debug/batch.pkl', 'rb'))\n",
    "loghs_hpc = pickle.load(open(f'{base_path}/data/epic_debug/loghs.pkl', 'rb'))\n",
    "\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    loghs_epic1 = predict_batch(model, batch_hpc)\n",
    "\n",
    "assert torch.allclose(torch.tensor(loghs_hpc).float(), torch.tensor(loghs_epic1).float(), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Data loading okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DebugImagesDataset(Dataset):\n",
    "    def __init__(self, data_images, transform):\n",
    "        self.data_images = data_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data_images.iloc[index][\"distfilename\"]\n",
    "\n",
    "        #try:\n",
    "        img_np = imread(f\"{base_path}/data/epic_debug/{img_name}\")\n",
    "        #except:\n",
    "            #img_np = imread(f\"{base_path}/BiHealth/Data/EPICImages_PoorQuality/{img_name}\")\n",
    "\n",
    "        img_pil = PIL.Image.fromarray(img_np)\n",
    "\n",
    "        try:\n",
    "            img_tensor = self.transform(img_pil)\n",
    "        except:\n",
    "            print(img_name)\n",
    "\n",
    "        return img_name, img_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_names, img_tensors = zip(*batch)\n",
    "    img_names = list(img_names)\n",
    "    img_tensors = torch.stack(img_tensors)\n",
    "\n",
    "    return Batch(img_tensors, None, img_names)\n",
    "\n",
    "def predict_batch(model, img_batch):\n",
    "    loghs = model(img_batch)[\"head_outputs\"][\"logits\"].detach().cpu().numpy()\n",
    "    return loghs\n",
    "\n",
    "corrupted_files = [\"0AIULA8E31FVNEXW_epiceye07142.png\", \n",
    "                   \"0AIULA8E315UZ3KA_epiceye03519.png\", \n",
    "                   \"0AIULA8E3354WXMB_epiceye03739.png\",\n",
    "                   \"0AIULA8E32JU9I3E_epiceye00148.png\",\n",
    "                   \"0AIULA8E3354JOZ0_epiceye06941.png\",\n",
    "                  \"0AIULA8E315XYVCL_epiceye05788.png\",\n",
    "                   \"0AIULA8E315WMUA2_epiceye05000.png\",\n",
    "                  \"0AIULA8E32JRQLLF_epiceye01039.png\",\n",
    "                  \"0AIULA8E329S6BCD_epiceye02546.png\",\n",
    "                  \"0AIULA8E31RCZB57_epiceye00155.png\",\n",
    "                  \"0AIULA8E31REEET3_epiceye05711.png\",\n",
    "                   \"0AIULA8E32SJJSL9_epiceye03063.png\",\n",
    "                  \"0AIULA8E31FQZ4OO_epiceye06657.png\",\n",
    "                   \"0AIULA8E1HIF7IFB_epiceye05427.png\",\n",
    "                  \"0AIULA8E32SDCBXM_epiceye00289.png\",\n",
    "                  \"0AIULA8E1KEKKV8Y_epiceye05179.png\"]\n",
    "\n",
    "#data_images = pd.read_stata(f\"{base_path}/BiHealth/Data/StudyData/BiHealth_20230313_Long.dta\").query(\"distfilename!=@corrupted_files\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing PNG images\n",
    "folder_path = f'{base_path}/data/epic_debug/'\n",
    "\n",
    "# List to hold image file names\n",
    "image_files = ['4307603_21015_0_0.png',\n",
    " '3507617_21016_0_0.png',\n",
    " '5566470_21015_1_0.png',\n",
    " '4321401_21015_1_0.png',\n",
    " '5080740_21016_0_0.png',\n",
    " '1899794_21015_0_0.png',\n",
    " '3728190_21016_0_0.png',\n",
    " '3667567_21016_1_0.png',\n",
    " '3298799_21016_0_0.png',\n",
    " '5293867_21015_0_0.png']\n",
    "\n",
    "# Create a DataFrame with the image file names\n",
    "data_images = pd.DataFrame(image_files, columns=['distfilename'])\n",
    "data_images\n",
    "\n",
    "dataset = DebugImagesDataset(data_images, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=10, collate_fn=collate_fn, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_epic = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_batch(batch):\n",
    "    # Assuming batch_epic.data is your tensor\n",
    "    # Convert it to numpy and transpose the axes for plotting\n",
    "    # From (batch_size, channels, height, width) to (batch_size, height, width, channels)\n",
    "    images = batch.data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    # Normalization parameters\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    # Reverse the normalization process\n",
    "    images = std * images + mean\n",
    "    images = np.clip(images, 0, 1)  # Ensuring the pixel values are in [0, 1]\n",
    "\n",
    "    # Set the number of images you want to display per row\n",
    "    images_per_row = 5\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    num_rows = len(images) // images_per_row + int(len(images) % images_per_row > 0)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    plt.figure(figsize=(20, 4 * num_rows))\n",
    "    for i, img in enumerate(images, 1):\n",
    "        plt.subplot(num_rows, images_per_row, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(batch_epic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(batch_hpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Data loading & inference okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ckpt = torch.load(f'{base_path}/BiHealth/ckpts_RetFound/4.ckpt', map_location='cpu')\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval();\n",
    "model.to(\"cuda\")\n",
    "\n",
    "with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    batch_epic.data = batch_epic.data.to(\"cuda\")\n",
    "    loghs_epic2 = predict_batch(model, batch_epic)\n",
    "\n",
    "assert torch.allclose(torch.tensor(loghs_hpc).float(), torch.tensor(loghs_epic2).float(), atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(loghs_hpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(loghs_epic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EPICImagesDataset(Dataset):\n",
    "    def __init__(self, data_images, transform):\n",
    "        self.data_images = data_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data_images.iloc[index][\"distfilename\"]\n",
    "\n",
    "        try:\n",
    "            img_np = imread(f\"{base_path}/BiHealth/Data/EPICImages/{img_name}\")\n",
    "        except:\n",
    "            img_np = imread(f\"{base_path}/BiHealth/Data/EPICImages_PoorQuality/{img_name}\")\n",
    "\n",
    "        img_pil = PIL.Image.fromarray(img_np)\n",
    "\n",
    "        try:\n",
    "            img_tensor = self.transform(img_pil)\n",
    "        except:\n",
    "            print(img_name)\n",
    "\n",
    "        return img_name, img_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_names, img_tensors = zip(*batch)\n",
    "    img_names = list(img_names)\n",
    "    img_tensors = torch.stack(img_tensors)\n",
    "\n",
    "    return Batch(img_tensors, None, img_names)\n",
    "\n",
    "def predict_batch(model, img_batch):\n",
    "    loghs = model(img_batch)[\"head_outputs\"][\"logits\"].detach().cpu().numpy()\n",
    "    return loghs\n",
    "\n",
    "corrupted_files = [\"0AIULA8E31FVNEXW_epiceye07142.png\", \n",
    "                   \"0AIULA8E315UZ3KA_epiceye03519.png\", \n",
    "                   \"0AIULA8E3354WXMB_epiceye03739.png\",\n",
    "                   \"0AIULA8E32JU9I3E_epiceye00148.png\",\n",
    "                   \"0AIULA8E3354JOZ0_epiceye06941.png\",\n",
    "                  \"0AIULA8E315XYVCL_epiceye05788.png\",\n",
    "                   \"0AIULA8E315WMUA2_epiceye05000.png\",\n",
    "                  \"0AIULA8E32JRQLLF_epiceye01039.png\",\n",
    "                  \"0AIULA8E329S6BCD_epiceye02546.png\",\n",
    "                  \"0AIULA8E31RCZB57_epiceye00155.png\",\n",
    "                  \"0AIULA8E31REEET3_epiceye05711.png\",\n",
    "                   \"0AIULA8E32SJJSL9_epiceye03063.png\",\n",
    "                  \"0AIULA8E31FQZ4OO_epiceye06657.png\",\n",
    "                   \"0AIULA8E1HIF7IFB_epiceye05427.png\",\n",
    "                  \"0AIULA8E32SDCBXM_epiceye00289.png\",\n",
    "                  \"0AIULA8E1KEKKV8Y_epiceye05179.png\"]\n",
    "\n",
    "data_images = pd.read_stata(f\"{base_path}/BiHealth/Data/StudyData/BiHealth_20230313_Long.dta\").query(\"distfilename!=@corrupted_files\")\n",
    "dataset = EPICImagesDataset(data_images, transform)\n",
    "#dataset = EPICImagesDataset(data_images, transform, base_path, num_workers=4, cache_size=100)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=48, collate_fn=collate_fn, drop_last=False)\n",
    "model = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [4]#5, 7, 9, 10, 20]\n",
    "\n",
    "tta_iterations = 1\n",
    "\n",
    "metadata = []\n",
    "for partition in tqdm(partitions):\n",
    "    # instantiate cktp here\n",
    "    ckpt = torch.load(f'{base_path}/BiHealth/ckpts_RetFound/{partition}.ckpt', map_location='cpu')\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval();\n",
    "    model.to(\"cuda\")\n",
    "    for iteration in tqdm(range(tta_iterations)): \n",
    "        for i, img_batch in tqdm(enumerate(list(dataloader))):\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    img_names = img_batch.names\n",
    "                    img_batch.data = img_batch.data.to(\"cuda\")\n",
    "                    loghs = predict_batch(model, img_batch)\n",
    "                    for img_name, logh in zip(img_names, loghs):\n",
    "                        metadata.append({\"partition\": partition, \"img_name\": img_name, \"iteration\": iteration, \"loghs\": logh})\n",
    "                    if i>=100: break\n",
    "                    #torch.cuda.empty_cache()\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_loghs(metadata_df):\n",
    "    for i in tqdm(range(len(metadata_df))):\n",
    "        if i==0: print(metadata_df.iloc[i].name, metadata_df.iloc[i].loghs.mean(), np.isnan(metadata_df.iloc[i].loghs).sum())\n",
    "        if np.isnan(metadata_df.iloc[i].loghs).sum()>0:\n",
    "            print(metadata_df.iloc[i].name, metadata_df.iloc[i].loghs.mean(), np.isnan(metadata_df.iloc[i].loghs).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nan_loghs(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loghs_epic.mean(), np.isnan(loghs_epic).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.reset_index(drop=True).to_feather(f\"{base_path}/data/predictionstta_231117_fixed.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df_old = pd.read_feather(f\"{base_path}/data/predictionstta_231117.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nan_loghs(metadata_df_old)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-python310]",
   "language": "python",
   "name": "conda-env-.conda-python310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
