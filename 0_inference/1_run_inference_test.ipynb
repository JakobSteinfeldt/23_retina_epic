{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/jakobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_md = pd.read_csv(f\"{base_path}/BiHealth/onnx/endpoints.csv\").drop(columns=\"Unnamed: 0\").set_index(\"endpoint\")#[[\"endpoint\", \"eligable\", \"n\", \"freq\", \"phecode\", \"phecode_string\", \"phecode_category\", \"sex\"]]\n",
    "endpoints_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from socket import gethostname\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchmetrics\n",
    "\n",
    "from retinalrisk.models.supervised import (\n",
    "    ImageTraining\n",
    ")\n",
    "from retinalrisk.modules.head import MLPHead\n",
    "\n",
    "def setup_training():\n",
    "    \n",
    "    def get_head(num_head_features, num_endpoints):\n",
    "\n",
    "        cls = MLPHead\n",
    "\n",
    "        return cls(\n",
    "            num_head_features,\n",
    "            num_endpoints,\n",
    "            incidence=None,\n",
    "            dropout=0.5,\n",
    "            gradient_checkpointing=False,\n",
    "            num_hidden =512, \n",
    "            num_layers =2,\n",
    "            #loss=None,\n",
    "        )\n",
    "    \n",
    "    base_path = \"/home/jakobs\"\n",
    "    x = torch.load(f\"{base_path}/BiHealth/ckpts/ckpt_partition_4\")\n",
    "    losses = x['hyper_parameters'][\"losses\"]\n",
    "    label_mapping = x['hyper_parameters'][\"label_mapping\"]\n",
    "    incidence_mapping = x['hyper_parameters'][\"incidence_mapping\"]\n",
    "\n",
    "    #encoder = tv.models.__dict__[args.model.encoder](pretrained=args.model.pretrained)\n",
    "    weights = 'DEFAULT' \n",
    "    encoder = tv.models.__dict__[\"convnext_small\"](weights=weights) \n",
    "    #print(encoder)\n",
    "\n",
    "    outshape = 768 \n",
    "\n",
    "    setattr(encoder.classifier, '2', torch.nn.Identity())\n",
    "\n",
    "    head = get_head(num_head_features=768, num_endpoints = 1171)\n",
    "\n",
    "    model = ImageTraining(encoder=encoder, head=head, losses=None, label_mapping=label_mapping, incidence_mapping=None, task=\"tte\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import PIL\n",
    "from typing import Union\n",
    "from random import choice\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.io import imread\n",
    "\n",
    "crop_ratio = [0.66]\n",
    "img_size_to_gpu = 420 # 420\n",
    "\n",
    "class AdaptiveRandomCropTransform(nn.Module):\n",
    "    def __init__(self,\n",
    "                 crop_ratio: Union[list,float],\n",
    "                 out_size: int,\n",
    "                 interpolation=InterpolationMode.BILINEAR):\n",
    "        super().__init__()\n",
    "        self.crop_ratio = crop_ratio\n",
    "        self.out_size = out_size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, sample):\n",
    "        input_size = min(sample.size)\n",
    "        if isinstance(self.crop_ratio, list):\n",
    "            crop_ratio = choice(self.crop_ratio)\n",
    "        else:\n",
    "            crop_ratio = self.crop_ratio\n",
    "\n",
    "        crop_size = int(crop_ratio * input_size)\n",
    "        if crop_size < self.out_size:\n",
    "            crop_size = tv.transforms.transforms._setup_size(self.out_size,\n",
    "                                                             error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(sample, crop_size)\n",
    "            return TF.crop(sample, i, j, h, w)\n",
    "        else:\n",
    "            crop_size = tv.transforms.transforms._setup_size(crop_size,\n",
    "                                                             error_msg=\"Please provide only two dimensions (h, w) for size.\")\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(sample, crop_size)\n",
    "            cropped = TF.crop(sample, i, j, h, w)\n",
    "        out = TF.resize(cropped, self.out_size, self.interpolation)\n",
    "                        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to apply\n",
    "transform = transforms.Compose([\n",
    "    AdaptiveRandomCropTransform(crop_ratio=crop_ratio,\n",
    "                                out_size=img_size_to_gpu,\n",
    "                                interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop(size=384),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "                    \n",
    "])\n",
    "\n",
    "#invTrans = transforms.Compose([ \n",
    "#    transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), \n",
    "#    transforms.Normalize(mean = [ -0.485, -0.456, -0.406],  std = [ 1., 1., 1. ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EPICImagesDataset(Dataset):\n",
    "    def __init__(self, data_images, transform):\n",
    "        self.data_images = data_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data_images.iloc[index][\"distfilename\"]\n",
    "\n",
    "        try:\n",
    "            img_np = imread(f\"{base_path}/BiHealth/Data/EPICImages/{img_name}\")\n",
    "        except:\n",
    "            img_np = imread(f\"{base_path}/BiHealth/Data/EPICImages_PoorQuality/{img_name}\")\n",
    "\n",
    "        img_pil = PIL.Image.fromarray(img_np)\n",
    "\n",
    "        try:\n",
    "            img_tensor = self.transform(img_pil)\n",
    "        except:\n",
    "            print(img_name)\n",
    "\n",
    "        return img_name, img_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_names, img_tensors = zip(*batch)\n",
    "    img_names = list(img_names)\n",
    "    img_tensors = torch.stack(img_tensors)\n",
    "    return img_names, img_tensors\n",
    "\n",
    "def predict_batch(model, img_batch):\n",
    "    loghs = model(img_batch)[\"head_outputs\"][\"logits\"].detach().cpu().numpy()\n",
    "    return loghs\n",
    "\n",
    "corrupted_files = [\"0AIULA8E31FVNEXW_epiceye07142.png\", \n",
    "                   \"0AIULA8E315UZ3KA_epiceye03519.png\", \n",
    "                   \"0AIULA8E3354WXMB_epiceye03739.png\",\n",
    "                   \"0AIULA8E32JU9I3E_epiceye00148.png\",\n",
    "                   \"0AIULA8E3354JOZ0_epiceye06941.png\",\n",
    "                  \"0AIULA8E315XYVCL_epiceye05788.png\",\n",
    "                   \"0AIULA8E315WMUA2_epiceye05000.png\",\n",
    "                  \"0AIULA8E32JRQLLF_epiceye01039.png\",\n",
    "                  \"0AIULA8E329S6BCD_epiceye02546.png\",\n",
    "                  \"0AIULA8E31RCZB57_epiceye00155.png\",\n",
    "                  \"0AIULA8E31REEET3_epiceye05711.png\",\n",
    "                   \"0AIULA8E32SJJSL9_epiceye03063.png\",\n",
    "                  \"0AIULA8E31FQZ4OO_epiceye06657.png\",\n",
    "                   \"0AIULA8E1HIF7IFB_epiceye05427.png\",\n",
    "                  \"0AIULA8E32SDCBXM_epiceye00289.png\",\n",
    "                  \"0AIULA8E1KEKKV8Y_epiceye05179.png\"]\n",
    "\n",
    "data_images = pd.read_stata(f\"{base_path}/BiHealth/Data/StudyData/BiHealth_20230313_Long.dta\").query(\"distfilename!=@corrupted_files\")\n",
    "dataset = EPICImagesDataset(data_images, transform)\n",
    "#dataset = EPICImagesDataset(data_images, transform, base_path, num_workers=4, cache_size=100)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False, num_workers=32, collate_fn=collate_fn, drop_last=False)\n",
    "model = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [4]\n",
    "\n",
    "metadata = []\n",
    "for iteration in tqdm(range(10)):\n",
    "    for partition in tqdm(partitions):\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            ckpt = torch.load(f\"{base_path}/BiHealth/ckpts/ckpt_partition_{partition}\")\n",
    "            model.encoder.load_state_dict({k[8:]:v for k, v in ckpt[\"state_dict\"].items() if \"encoder\" in k}, strict=True)\n",
    "            model.head.load_state_dict({k[5:]:v for k, v in ckpt[\"state_dict\"].items() if \"head\" in k}, strict=True)\n",
    "            model.eval();\n",
    "            model.to(\"cuda\")\n",
    "            # instantiate cktp here\n",
    "            for img_names, img_batch in tqdm(dataloader):\n",
    "                img_batch = img_batch.to(\"cuda\")\n",
    "                loghs = predict_batch(model, img_batch)\n",
    "                for img_name, logh in zip(img_names, loghs):\n",
    "                    metadata.append({\"partition\": partition, \"img_name\": img_name, \"iteration\": iteration, \"loghs\": logh})\n",
    "                del img_batch\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.reset_index(drop=True).to_feather(f\"{base_path}/data/predictionstte_test_230321.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = metadata_df[\"loghs\"].apply(pd.Series)\n",
    "wide_df.columns = endpoints_md.index\n",
    "wide_df\n",
    "\n",
    "predictions = metadata_df.merge(wide_df, how=\"left\", left_index=True, right_index=True).drop(\"loghs\", axis=1)\n",
    "\n",
    "predictions.to_feather(f\"{base_path}/data/predictionsttewide_test_230321.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-python310]",
   "language": "python",
   "name": "conda-env-.conda-python310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
